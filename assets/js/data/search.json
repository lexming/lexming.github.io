[ { "title": "File system loop with Btrfs and NFS", "url": "/posts/btrfs-nfs-file-system-loop/", "categories": "linux", "tags": "storage", "date": "2023-01-15 16:00:00 +0100", "snippet": "I got the following error on a regular find command looking for files in thedata directory of my Nextcloud instance:$ find /media/nextcloud/ -name \"potato\"find: File system loop detected; ‘/media/n...", "content": "I got the following error on a regular find command looking for files in thedata directory of my Nextcloud instance:$ find /media/nextcloud/ -name \"potato\"find: File system loop detected; ‘/media/nextcloud/log’ is part of the same file system loop as ‘/media/nextcloud/’.find: File system loop detected; ‘/media/nextcloud/data’ is part of the same file system loop as ‘/media/nextcloud/’.find: File system loop detected; ‘/media/nextcloud/scripts’ is part of the same file system loop as ‘/media/nextcloud/’.This error about a loop in the file system is very unexpected. Such loops arecommonly caused by symbolic links pointing back to some parent folder (seebelow) and I know that there are none in that mount at /media/nextcloudbecause Nextcloud does not support symlinks..└── root_folder └── sub_folder_1 └── sub_folder_2 &gt; /root_folder └── sub_folder_1 └─ (...)Inodes in Btrfs and NFSThe reason that leads to this file system loop error in /media/nextcloud isthat its 3 sub-folders data, log and scripts have the same inode number,understandanbly tricking find into a file system loop.$ ls -i /media/nextcloud256 data 256 log 256 scriptsThe common inode number between these folders originates from the underlyingBtrfs file system of this mount. Eachsub-folder data, log and scripts is aBtrfs subvolumeinside another subvolume /media/nextcloud and one characteristic of Btrfssubvolumes is that they all have the same256 inode number.This single inode number is usually not an issue for subvolumes locally mountedwith Btrfs. The filesystem allocates a separate device number for eachsubvolume, which allows to distinguish each sub-folder even if only the rootsubvolume is mounted.¹So why the error then? In this case, the root Btrfs subvolume/media/nextcloud is mounted over the network withNFS and its child subvolumes are accessiblethrough this single NFS mount. The NFS server of /media/nextcloud alreadysets explictly the filesystem ID of the exported subvolume with theoption fsid. However, this filesystemID does not help with the nested subvolumes, NFS does not expose anything elsethan the inode number for those and hence, the sub-folders data, log andscripts are left undistinguishable.SolutionThe solution is to export an NFS mount for each subfolder, with their ownfsid, and mount each one of those on the client system:$ mount | grep nextcloudhelios4:/cloud on /media/nextcloud type nfs4 (rw,noatime,vers=4.2)helios4:/cloud_data on /media/nextcloud/data type nfs4 (rw,noatime,vers=4.2)helios4:/cloud_log on /media/nextcloud/log type nfs4 (rw,noatime,vers=4.2)helios4:/cloud_scripts on /media/nextcloud/scripts type nfs4 (rw,noatime,vers=4.2)The inodes of the sub-folders will still be the common 256, but find will nolonger report file system loops:$ ls -i /media/nextcloud/256 data 256 log 256 scripts$ find /media/nextcloud -name \"potato\"/media/nextcloud/scripts/potatoReferencesA lot more information in the two-part series of LWN.net: The Btrfs inode-number epic (part 1: the problem) The Btrfs inode-number epic (part 2: solutions)" }, { "title": "Make error: Argument list too long", "url": "/posts/error-argument-list-too-long/", "categories": "easybuild", "tags": "make", "date": "2022-10-27 18:00:00 +0200", "snippet": "I got the following error compiling nodejs v16.15.1 with GCC v10.3.0: touch /theia/scratch/brussel/vo/000/bvo00000/vsc10000/easybuild/install/skylake/build/nodejs/16.15.1/GCCcore-10.3.0/node-v16.1...", "content": "I got the following error compiling nodejs v16.15.1 with GCC v10.3.0: touch /theia/scratch/brussel/vo/000/bvo00000/vsc10000/easybuild/install/skylake/build/nodejs/16.15.1/GCCcore-10.3.0/node-v16.15.1/out/Release/obj.target/tools/v8_gypfiles/v8_compiler_for_mksnapshot.stamp make[1]: execvp: printf: Argument list too long make[1]: *** [/theia/scratch/brussel/vo/000/bvo00000/vsc10000/easybuild/install/skylake/build/nodejs/16.15.1/GCCcore-10.3.0/node-v16.15.1/out/Release/obj.target/tools/v8_gypfiles/libv8_base_without_compiler.a] Error 127 rm 5bbfd6c4a8f8ff3451772da37fa142cac4520fdc.intermediate c551182bfbb57b84e78564bf84990af2837af887.intermediate make[1]: Leaving directory `/theia/scratch/brussel/vo/000/bvo00000/vsc10000/easybuild/install/skylake/build/nodejs/16.15.1/GCCcore-10.3.0/node-v16.15.1/out’ make: *** [node] Error 2The error printf: Argument list too long is not related to the GCC compilerbut the build with make. The issue is caused by some command that becomes toolong, not only because of a large number of arguments but also because of a toolong argument line. I typically hit this type of error in nodejs andQt5. For instance, nodejs is known to have long linkercommands aggregating many objects in static binaries(Gentoo#809935).In my case, the main cause of this error is the path to the build directory:/theia/scratch/brussel/vo/000/bvo00000/vsc10000/easybuild/install/skylake/build/nodejs/16.15.1/GCCcore-10.3.0/node-v16.15.1The build directory is quite deep within the filesystem and, most importantly,it is long. Commands in the build process dealing with many files referencedwith their absolute paths will become too long and trigger this error.The solution is as simple as carrying out the build in a different directorywith a shorted absolute path. I exceptionally execute such builds in a temporarydirectory in /tmp. EasyBuild allows to change thelocation of the build directory for a single build by just setting the commandline option --buildpath." }, { "title": "Navigating through the parent directory of symlinks in bash", "url": "/posts/parent-directories-symlinks/", "categories": "linux", "tags": "bash", "date": "2022-10-16 12:00:00 +0200", "snippet": "Changing directory cd to a symlinkedfolder can have unexpected consequence on how certain commands in bash behave.For instance, assume that we have the following file structure whereactive_project ...", "content": "Changing directory cd to a symlinkedfolder can have unexpected consequence on how certain commands in bash behave.For instance, assume that we have the following file structure whereactive_project is a symlink to a folder nested in another file tree:.├── data│   ├── project1│   │   ├── 01-01.csv│   │   ├── 01-02.csv│   │   └── 01-03.csv│   └── project2│   ├── 02-aa.csv│   └── 02-ab.csv└── active_project -&gt; data/project24 directories, 5 filesOnce we change directory into active_project, the commands ls and cd willinterpret .. differently and show different contents for its parent folder:$ cd active_project/$ ls ..project1 project2$ (cd .. &amp;&amp; ls)data projectThis different outcome is due to how cd works. The command cd is a built-incommand in bash and as such, it works with the path reported by pwd which isaware of the actual file path followed by the user. Hence, cd will follow thesymlink backwards. On the other hand, the command ls is a regular executableand it works with the real path or physical path of the current folder.Therefore, in the previous example, ls will jump to the actual folder thatcontains project2.TAB completion can also behave differently depending on the default settings ofbash in your Linux distribution or if extra bash completions are included. Thebuilt-in completion for cd in bash should follow the symlink on ..backwards, as cd does. If that’s not the case, an external bash completionmight be interfering. You can switch back to the built-in completion for cdwith the following command in your .bashrc:complete -r cd" }, { "title": "Kernel EFI stub boot with Btrfs root", "url": "/posts/kernel-efi-stub-btrfs/", "categories": "linux", "tags": "gentoo", "date": "2022-07-18 20:00:00 +0200", "snippet": "We can directly boot into the Linux kernel without any secondary bootloader(e.g. GRUB) by enabling support in thekernel for the EFI bootloader.Processor type and features ---&gt; [*] EFI runtim...", "content": "We can directly boot into the Linux kernel without any secondary bootloader(e.g. GRUB) by enabling support in thekernel for the EFI bootloader.Processor type and features ---&gt; [*] EFI runtime service support [*] EFI stub support [ ] EFI mixed-mode supportThe EFI bootloader also supports initramfsto handle more complex tasks early in the boot process. Typically, mountingimportant filesystems that need any kernel modules, mounting encryptedfilesystems or loading firmware for the kernel drivers.General setup ---&gt; [*] Initial RAM filesystem and RAM disk (initramfs/initrd) support () Initramfs source file(s)Processor type and features ---&gt; [ ] Built-in kernel command lineDevice Drivers ---&gt; Generic Driver Options ---&gt; [*] Maintain a devtmpfs filesystem to mount at /devThe initramfs image does not have to be embedded into the kernel with theoption Initramfs source file(s), as it will be directly loaded by the EFIbootloader. Moreover, the initramfs will also take care of passing theappropriate command line options to the kernel, so it is not necessary todefine any in the Built-in kernel command line configuration.EFI bootloader setup Compile the Linux kernel $ cd /usr/src/linux $ make &amp;&amp; make modules_install Copy the kernel image to the EFI System Partition $ cp arch/x86/boot/bzImage /boot/efi/EFI/Gentoo/bzImage-x.y.z.efi Generate the initramfs with dracut(optional: compressed with ZSTD) $ dracut --kver=x.y.z-gentoo --zstd Copy the new initramfs image to the EFI System Partition $ cp /boot/initramfs-x.y.z-gentoo.img /boot/efi/EFI/Gentoo/initramfs-x.y.z.img Generate a new boot entry for this new kernel in the EFI bootloader $ efibootmgr --create --disk /dev/nvme0n1 --part 1 --label 'Gentoo' --loader '\\efi\\gentoo\\bzImage-x.y.z.efi' --unicode 'initrd=\\efi\\gentoo\\initramfs-x.y.z.img' In this example, the partition /dev/nvme0n1p1 is theEFI System Partitionand paths are relative to its root. Btrfs root partitionThe previous setup allows to easily boot from aBtrfs root partition by using the initramfs tomount it. Btrfs can be compiled into the kernel or added as module.File systems ---&gt; &lt;*/M&gt; Btrfs filesystemDracut will auto-detect the format of the root partition as long as it isalready mounted. Alternatively, btrfs can be manually added to the initramfsimage withfilesystems+=\" btrfs \"" }, { "title": "Bootstrapping a headless Raspberry Pi", "url": "/posts/headless-raspberry-pi/", "categories": "linux", "tags": "debian", "date": "2022-07-09 20:00:00 +0200", "snippet": "Nowadays, thanks to the rpi-manager from Raspberry PiOS, it is quite trivial to bootstrap aRaspberry Pi into a working system in your network with no other interactionthan inserting the SD card and...", "content": "Nowadays, thanks to the rpi-manager from Raspberry PiOS, it is quite trivial to bootstrap aRaspberry Pi into a working system in your network with no other interactionthan inserting the SD card and plugging in the device. No peripherals and nodisplay needed. Insert a blank SD card in your computer and launch rpi-imager Select the OS image for your Raspberry Pi Click the cogwheel button on the bottom right Enable SSH Set a custom username and password Write the image into the SD card, insert the card into your Raspberry Pi andboot it up. The Raspberry Pi will get its IP address by DHCP. Check your network routerto get its IP. Connect with SSH using your user/passwordAt this point, if you can login with SSH, your Raspberry Pi is running andconnected. The following are some optional configurations that I usually applyto the Raspberry Pis in my network.Key-based SSH authentication Copy your SSH key to the Raspberry Pi ssh-copy-id -i .ssh/&lt;some_id_rsa&gt; username@&lt;rpi-hostname-or-ip&gt; Once you can login with your SSH key, disable SSH connections with password PasswordAuthentication no Disable radiosWi-Fi and Bluetooth might not be necessary if your Raspberry Pi is connectedwith Ethernet. The boot configuration file is located at /boot/config.txt in Raspberry PiOS (Raspbian) or /boot/firmware/config.txt in Ubuntu. Append the following settings to the boot configuration file to disableWi-Fi and Bluetooth dtoverlay=disable-wifi dtoverlay=disable-bt Disable audio/videoAudio and video are usually not needed on a headless system. On-board audio,HDMI output and video acceleration can be disabled with boot configurationoptions. Append the following settings to the boot configuration file to disablethe on-board audio (HDMI audio will still work if the video core driver isloaded) dtparam=audio=off Remove from the boot configuration file any line loading the video driveroverlay to disable video acceleration and both video and audio signals overHDMI. For instance, the overlay for the video driver for the Raspberry Pi 4is loaded with dtoverlay=vc4-kms-v3d. Append the following settings to the boot configuration file to switch offthe HDMI ports and disable any framebuffers on boot: hdmi_ignore_hotplug=1 max_framebuffers=0 Enable a watchdogThe Raspberry Pi board has a watchdog timer.This can be used to automatically reboot the system in case of problems. Forinstance, if the system hangs due to out-of-memory errors. Append the following settings to the boot configuration file to enable thewatchdog timer dtparam=watchdog=on Enable the watchdog service in systemd RuntimeWatchdogSec=15s # hardware limitation of RPi RebootWatchdogSec=10min #KExecWatchdogSec=0 WatchdogDevice=/dev/watchdog The watchdog timer in the Raspberry Pi is limited to 15 seconds (max) by hardware Reload config and reboot $ systemctl daemon-reload $ systemctl reboot Check after boot that watchdog is active # dmesg | grep watchdog [ 1.792121] bcm2835-wdt bcm2835-wdt: Broadcom BCM2835 watchdog timer [ 3.552959] systemd[1]: Using hardware watchdog 'Broadcom BCM2835 Watchdog timer', version 0, device /dev/watchdog [ 3.563591] systemd[1]: Set hardware watchdog to 15s. " }, { "title": "Edit systemd units", "url": "/posts/systemd-edit-units/", "categories": "linux", "tags": "systemd", "date": "2022-06-15 18:00:00 +0200", "snippet": "Unit files in systemd can be mended without modifyingthe actual unit file. This is useful to apply custom modifications to systemdservices that will survive updates of the package, as system update...", "content": "Unit files in systemd can be mended without modifyingthe actual unit file. This is useful to apply custom modifications to systemdservices that will survive updates of the package, as system updates mightreset the original unit file.For instance, at the time of writting, issuehaveged#41 was still present inArmbian 22.05.1 Bullseye. We can manually apply the fix to haveged service fromcommit haveged@159dcde2with the following steps: Edit haveged service file $ systemctl edit haveged.service Add any modified or new parameters to the unit file ### Editing /etc/systemd/system/haveged.service.d/override.conf ### Anything between here and the comment below will become the new contents of the file [Service] SystemCallFilter=@system-service SystemCallFilter=~@mount SystemCallErrorNumber=EPERM ### Lines below this comment will be discarded Keep in mind to always specify the section of the parameters. " }, { "title": "Git repos in Nextcloud", "url": "/posts/git-repo-nextcloud/", "categories": "nextcloud", "tags": "git", "date": "2022-05-08 20:00:00 +0200", "snippet": "The usual solution to put git repositories on a remote server is to transferand sync them with SSH.However, in cases where setting up the SSH connection is troublesome (e.g.lack of SSH server or la...", "content": "The usual solution to put git repositories on a remote server is to transferand sync them with SSH.However, in cases where setting up the SSH connection is troublesome (e.g.lack of SSH server or lack of keys in the client system), it can be useful touse Nextcloud as an alternative to mirror gitrepositories and/or syncronize them accross computers. Nextcloud is not designed to be used as a source code management tool, if youneed one check Github,GitLab or Gitea.Putting a git repository in Nextcloud can be achieved quite easily by placingthe bare repositoryof your repo in a local folder and sync it with your Nextcloud instance.Syncronization speed is on the low end, so this solution is not meant forreal-time syncronization of repositories with a lot of activity, but is aviable option for small private repositories. Sync a local folder with your Nextcloud instance using any of theNextcloud clients. In the following, thisfolder will be ~/Public/Nextcloud Put a git bare repository in the folder synced with Nextcloud Exporting the bare repository of an existing repository $ git clone --bare my_project ~/Public/Nextcloud/my_project.git Inititalizing a new git repository from a local bare repository $ git init --bare ~/Public/Nextcloud/my_project.git $ git clone ~/Public/Nextcloud/my_project.git Check tracked remotes in your repository $ cd my_project $ git remote -v origin /home/user/Public/Nextcloud/my_project.git (fetch) origin /home/user/Public/Nextcloud/my_project.git (push) Use a different name than origin for the remote repository in Nextcloudto add it as a non-default remote (e.g. in case it is used as backup). " }, { "title": "Cron jobs with systemd", "url": "/posts/systemd-cron-units/", "categories": "linux", "tags": "systemd", "date": "2022-04-05 18:00:00 +0200", "snippet": "Systemd has timers, which can be used as an alternativeto cron to schedule jobs. The timersinfrastructure is quite powerful and is described in detail elsewhere.⁽¹⁾⁽²⁾The following shows common tim...", "content": "Systemd has timers, which can be used as an alternativeto cron to schedule jobs. The timersinfrastructure is quite powerful and is described in detail elsewhere.⁽¹⁾⁽²⁾The following shows common timer examples to execute an arbitrary service unitmyunit and basic commands to control the timers.Monotonic timer[Unit]Description=Run myunit.service every 60 minutes[Timer]OnBootSec=15minOnUnitActiveSec=60minUnit=myunit.service[Install]WantedBy=timers.targetRealtime timer[Unit]Description=Run myunit.service once a week at 2:00 AM[Timer]OnCalendar=Tue *-*-* 02:00:00Unit=myunit.service[Install]WantedBy=timers.targetLaunch and monitor the timerTimers can be added as a regular user or system-wide as root. Add the timer unit file $ systemctl edit --full myunit.timer Enable the timer (not the service unit) $ systemctl enable myunit.timer Monitor the active timers $ systemctl list-timers Timers can be manually triggered with the command systemctl startmyunit.timer." }, { "title": "Linker error: multiple definition of symbol", "url": "/posts/linker-error-multiple-definition/", "categories": "easybuild", "tags": "linker", "date": "2022-02-24 17:00:00 +0100", "snippet": "I got the following error from the linker ld.gold while linking libsumo ina build of SUMO with bindings toFOX-toolkit: g++ -fPIC -O2 -ftree-vectorize -march=native -fno-math-errno -std=c++11 -Wall...", "content": "I got the following error from the linker ld.gold while linking libsumo ina build of SUMO with bindings toFOX-toolkit: g++ -fPIC -O2 -ftree-vectorize -march=native -fno-math-errno -std=c++11 -Wall -pedantic -Wextra -fPIC -O3 -DNDEBUG -L/apps/PROJ/7.0.0-GCCcore-9.3.0/lib -L/apps/GDAL/3.0.4-foss-2020a-Python-3.8.2/lib -L/apps/GL2PS/1.4.2-GCCcore-9.3.0/lib64 -L/apps/GL2PS/1.4.2-GCCcore-9.3.0/lib -L/apps/FOX-Toolkit/1.6.57-GCCcore-9.3.0/lib64 -L/apps/FOX-Toolkit/1.6.57-GCCcore-9.3.0/lib -L/apps/Xerces-C++/3.2.3-GCCcore-9.3.0/lib64 -L/apps/Python/3.8.2-GCCcore-9.3.0/lib -L/apps/Java/11.0.2/lib -L/apps/FFTW/3.3.8-gompi-2020a/lib -L/apps/ScaLAPACK/2.1.0-gompi-2020a/lib -L/apps/OpenBLAS/0.3.9-GCC-9.3.0/lib -L/apps/GCCcore/9.3.0/lib64 -L/apps/GCCcore/9.3.0/lib -shared -o build/SUMO/1.0.0/foss-2020a-Python-3.8.2/sumo-1.0.0/tools/libsumo/_libsumo.so CMakeFiles/_libsumo.dir///tools/libsumo/libsumoPYTHON_wrap.cxx.o -Wl,–whole-archive ../netload/libnetload.a ../microsim/libmicrosim.a ../microsim/cfmodels/libmicrosim_cfmodels.a ../microsim/lcmodels/libmicrosim_lcmodels.a ../microsim/devices/libmicrosim_devices.a ../microsim/output/libmicrosim_output.a ../microsim/pedestrians/libmicrosim_pedestrians.a ../microsim/trigger/libmicrosim_trigger.a ../microsim/actions/libmicrosim_actions.a ../microsim/traffic_lights/libmicrosim_traffic_lights.a ../mesosim/libmesosim.a ../traci-server/libtraciserver.a liblibsumostatic.a ../utils/emissions/libutils_emissions.a ../foreign/PHEMlight/cpp/libforeign_phemlight.a ../utils/vehicle/libutils_vehicle.a ../utils/distribution/libutils_distribution.a ../utils/shapes/libutils_shapes.a ../utils/options/libutils_options.a ../utils/xml/libutils_xml.a ../utils/geom/libutils_geom.a ../utils/common/libutils_common.a ../utils/importio/libutils_importio.a ../utils/iodevices/libutils_iodevices.a ../foreign/tcpip/libforeign_tcpip.a /apps/Xerces-C++/3.2.3-GCCcore-9.3.0/lib64/libxerces-c.so /apps/PROJ/7.0.0-GCCcore-9.3.0/lib/libproj.so -L/apps/FOX-Toolkit/1.6.57-GCCcore-9.3.0/lib -lFOX-1.6 -lX11 -lXext -lfreetype -lfontconfig -lXft -lXcursor -lXrender -lXrandr -lXfixes -lXi -ldl -lpthread -lrt -lm -lpthread -ljpeg -lpng -ltiff -lz -lbz2 -Wl,–no-whole-archive /apps/Python/3.8.2-GCCcore-9.3.0/lib/libpython3.8.so /apps/binutils/2.34-GCCcore-9.3.0/bin/ld.gold: error: /usr/lib64/libpthread_nonshared.a(pthread_atfork.oS): multiple definition of ‘__pthread_atfork’ /apps/binutils/2.34-GCCcore-9.3.0/bin/ld.gold: /usr/lib64/libpthread_nonshared.a(pthread_atfork.oS): previous definition here /apps/binutils/2.34-GCCcore-9.3.0/bin/ld.gold: error: /usr/lib64/libpthread_nonshared.a(pthread_atfork.oS): multiple definition of ‘pthread_atfork’ /apps/binutils/2.34-GCCcore-9.3.0/bin/ld.gold: /usr/lib64/libpthread_nonshared.a(pthread_atfork.oS): previous definition here collect2: error: ld returned 1 exit status make[2]: *** [build/SUMO/1.0.0/foss-2020a-Python-3.8.2/sumo-1.0.0/tools/libsumo/_libsumo.so] Error 1 make[2]: Leaving directory ‘build/SUMO/1.0.0/foss-2020a-Python-3.8.2/easybuild_obj’ make[1]: *** [src/libsumo/CMakeFiles/_libsumo.dir/all] Error 2 make[1]: Leaving directory ‘build/SUMO/1.0.0/foss-2020a-Python-3.8.2/easybuild_obj’ make: *** [all] Error 2The main errors from the linker are the multiple definition of 'symbol'errors, which are caused by duplicate LDFLAGS in the linker command. In thiscase, -lpthread is declared twice g++ […] -ldl -lpthread -lrt -lm -lpthread -ljpeg -lpng […]This error can be easily solved by removing the duplicate linker flag. However,depending on the underlying build system, this can be more or less involved. Ifthe build is done with make, it can be just a matter of manually editing thecorresponding Makefile.In this case, the build is based on CMake and hence, itis necessary to dig into its modules to find the spot where this duplicationhappens. Once the origin of the duplication is found, CMake provides theREMOVE_DUPLICATES method to clean all duplicates from a list.The following code snippet automatically removes duplicate elements inFOX_LIBRARY, which contains these flags in the build of SUMO(cmake_modules/FindFOX.cmake):separate_arguments(FOX_LIBRARY)list(REMOVE_DUPLICATES FOX_LIBRARY)string(REPLACE \";\" \" \" FOX_LIBRARY \"${FOX_LIBRARY}\")" }, { "title": "Poor man's history fuzzy finder", "url": "/posts/history-fuzzy-finder/", "categories": "linux", "tags": "bash", "date": "2022-02-19 16:00:00 +0100", "snippet": "Fuzzy finders are very usefultools to skim through long lists of results by filtering those items thatapproximately match a given string(s). In linux, we can get a fuzzy finder rightin the terminal...", "content": "Fuzzy finders are very usefultools to skim through long lists of results by filtering those items thatapproximately match a given string(s). In linux, we can get a fuzzy finder rightin the terminal with fzf.We can also make a very simple fuzzy finder by using grep in a recursivefunction in bash. The goal is to consecutively match the input text with eachand all pattern strings in the argument list. This can serve as a rudimentarysolution if fzf is not available in your system (or, you know, just for fun!:sunglasses:).Recursive grepThe recursive function will pick the top pattern string in the argument list,execute grep on the input to match that pattern, shift the arguments and passthe result to itself. I call this function rgrepfunction rgrep { if [ \"$1\" ]; then local pattern=\"$1\" if [ \"$2\" ]; then shift grep -- \"$pattern\" | rgrep \"$@\" else grep -- \"$pattern\" fi fi}The result is equivalent to perform a logical AND on all patterns with thecommand$ cat input.txt | grep 'pattern1' | grep 'pattern2' | grep 'pattern3'But with a simpler syntax, similar to a fuzzy finder$ cat input.txt | rgrep 'pattern1' 'pattern2' 'pattern3'History fuzzy finderOur new command rgrep can be easily applied to any command that prints lists of results, suchas history. For instance, I set hgrep as the following alias to easily find commands inmy history log$ alias hgrep='history | rgrep'Example:$ hgrep dnf openssl 611 dnf install openssl-devel 623 sudo dnf autoremove openssl-devel 1014 hgrep dnf openssl" }, { "title": "Sharing notes with Carnet and Nextcloud", "url": "/posts/sharing-notes-carnet-nextcloud/", "categories": "nextcloud", "tags": "", "date": "2022-02-09 12:00:00 +0100", "snippet": "Carnet is an open-source note taking app with a leanWYSIWYG interface that is very easy to use. Combined with its capability to useany Nextcloud instance as its storage backend,Carnet is a very com...", "content": "Carnet is an open-source note taking app with a leanWYSIWYG interface that is very easy to use. Combined with its capability to useany Nextcloud instance as its storage backend,Carnet is a very compelling solution for your self-hosted notes.Unfortunately, Carnet does not currently provide any mechanism to share noteswith other users in your Nextcloud -CarnetApp/CarnetNextcloud/#48.The simple workaround to this limitation is directly sharing the .sqd file ofthe note in Nextcloud with any other user. This sharing method can cause data loss in the shared note ifmultiple users edit the same note at the same time. In the case of simultaneousedits, the last user syncing will overwrite any new changes from other users. Locate the .sqd note file in your Carnet folder in Nextcloud(Documents/QuickNote by default) Share it with any other user in Nextcloud as usual The receiving user has to manually move the new .sqd file in theirNextcloud home folder into their Carnet folder (Documents/QuickNote bydefault) Carnet will automatically detect the new shared note and show it in theapplication interface" }, { "title": "Mirrorless camera as webcam in Linux", "url": "/posts/camera-as-webcam/", "categories": "linux", "tags": "fedora, audio, video", "date": "2022-01-22 16:00:00 +0100", "snippet": "Some modern camera models can be directly connected to the computer through USBand be detected as a webcam. That’s the easiest setup, but beware that this modeof operation might need special progra...", "content": "Some modern camera models can be directly connected to the computer through USBand be detected as a webcam. That’s the easiest setup, but beware that this modeof operation might need special programs/drivers with no support for Linux. Themore general approach is to use some video capture device that can take the HDMIoutput of the camera, stream it through USB and present itself as a USB webcam.Camera conversion to USB webcamIn my case, the camera is a Fujifilm X-E3 which can send clean video through itsHDMI output and I use Elgato Cam Link 4K to convert the video signal to USB.Once the camera is connected to the computer, it is recognized as any regularUSB webcam thanks to the open source uvcvideo kernel module. Linux kernelv5.14 fixed an issue with the reported pixel format by the CamLink, so any Linuxdistribution with a recent kernel (or backported patches) should work with thisdevice out of the box.Virtual webcam in OBS StudioOBS Studio is very useful to mix audio and videostreams in real time. For instance, I use it to sync the audio of my mic orcapture my computer desktop overlaying my cam in a corner of the stream.Since version 26.1 (Dec 2020), OBS natively provides a Virtual Camera output,which allows to catch its stream from any video chat application. OBS itselfwill appear as a new webcam device. Enabling this option requires the kernelmodule v4l2loopback, which might notbe installed by default.In Fedora 35, OBS Studio v26.1+ is available as a flatpak$ sudo flatpak install flathub com.obsproject.StudioThe kernel module v4l2loopback is available in the RPMFusionrepository. Once RPM Fusion is enabled in your system,install v4l2loopback with the following command:$ sudo dnf install kmod-v4l2loopbackOnce v4l2loopback is installed, restart OBS Studio and the new option StartVirtual Camera will appear below the Start Recording button.Note: Some video call applications limit the video resolution of the webcamdevice (e.g. MS Teams accepts 1280x720 max). In such a case, set the requiredresolution of the output stream in OBS in Settings &gt; Video &gt; Output (Scaled)Resolution.Virtual microphoneOne limitation of the Virtual Camera in OBS is that it only contains the videostream. If you want to use the audio stream from OBS in your video call as well,you will need to redirect its audio into a virtual mic. This can be done withPulseAudio by creatinga null-sink device as virtual speaker and remapping its monitor to a newaudio source as virtual mic. These two virtual audio devices allow to redirectthe audio from any application to any other application. Create virtual speaker $ pactl load-module module-null-sink sink_name=Virtual-Speaker sink_properties=device.description=Virtual-Speaker Remap monitor of virtual speaker as virtual mic $ pactl load-module module-remap-source source_name=Virtual-Mic master=Virtual-Speaker.monitor The advantage of this approach is that it does not depend on any specific audiodevice. The previous commands should work in any system with PulseAudio.Redirect audio in OBS to virtual speaker Set the monitoring device of your stream in OBS to Monitor of Virtual Speaker.This setting is located in Settings &gt; Audio &gt; Advanced &gt; Monitoring device. All audio sources of your stream in OBS should be real audio devices (i.e.the real mic) In the video call application: choose OBS Virtual Camera as webcam choose Virtual Mic as mic or, alternatively, set the Virtual Mic asdefault mic in PulseAudio " }, { "title": "Upgrade KBD67v2 to VIA compatible firmware", "url": "/posts/kbd67-via-firmware/", "categories": "linux", "tags": "keyboards", "date": "2022-01-09 16:00:00 +0100", "snippet": "Keyboards with VIA compatible firmwares can havetheir layouts easily configured with theVIA Configurator, a user-spaceapplication that can be run without any special permissions and that supportsLi...", "content": "Keyboards with VIA compatible firmwares can havetheir layouts easily configured with theVIA Configurator, a user-spaceapplication that can be run without any special permissions and that supportsLinux! :tada: This method removes the need to build and re-flash a new firmwareinto the keyboard each time that it has to be re-programmed, which is timeconsuming and requires tools not supporting Linux.Even though the factory firmware in my KBDfans KBD67v2is not VIA compatible, it is actually supported by VIA and they provide a newfirmware for this keyboard. So, let’s upgrade it: Download VIA compatible firmware fromhttps://caniusevia.com/docs/download_firmware/ The file for KBDv2 is kbdfans_kbd67_rev2_via.hex Flash VIA firmware to KDB67v2 with QMK Toolbox QMK Toolbox only works on Windows or Mac systems (Re-)Connect the keyboard in bootloader mode by pressing Space + Bwhile you plug its USB cable Launch QMK Toolbox Select kbdfans_kbd67_rev2_via.hex as Local file Select atmega32u4 as Microcontroller Select kbdfans/kbd67/rev2 as Keyboard Click Flash (disabled if the keyboard is not in bootloader mode, seestep 2.1) Download and installVIA Configurator With this new firmware, VIA Configurator now recognizes my KBD67v2 on launch andI can not only configure its multiple layout layers, but also control thebacklight settings. All changes are applied and stored in the keyboardon-the-fly as well." }, { "title": "KDE Konsole padding", "url": "/posts/konsole-padding/", "categories": "linux", "tags": "kde", "date": "2021-12-19 18:00:00 +0100", "snippet": "Konsole, KDE’s terminal emulator, supportschanging the padding space in its terminal window.This property can be changed from the graphical interface of Konsole sincev19.04. It is called Margins an...", "content": "Konsole, KDE’s terminal emulator, supportschanging the padding space in its terminal window.This property can be changed from the graphical interface of Konsole sincev19.04. It is called Margins and it can be found in:Settings &gt; Edit Current Profile &gt; Appearance (left toolbox) &gt; Miscellaneous (tab) &gt; MarginsAlternatively, the padding of the terminal window can also be set in yourprofile configuration file. Which also works in older versions of Konsolewithout the toggle in the GUI. Change the default padding by setting theTerminalMargin parameter to the desired value of pixels in the Generalsection of your profile file in ~/.local/share/konsole/[General]Name=lexmingParent=FALLBACK/TerminalColumns=120TerminalMargin=12TerminalRows=48" }, { "title": "Make systemd service wait for mount", "url": "/posts/systemd-service-wait-mount/", "categories": "linux", "tags": "systemd, storage", "date": "2021-11-18 16:00:00 +0100", "snippet": "Services in systemd can be set to wait on theavailability of any specific mount point in the local filesystem. This is veryuseful for NFS mounts as those require the network to be up and running, w...", "content": "Services in systemd can be set to wait on theavailability of any specific mount point in the local filesystem. This is veryuseful for NFS mounts as those require the network to be up and running, whichcan take a while.This feature is controlled through theRequiresMountsForoption in the unit file of the service.For instance, I use RequiresMountsFor in the fail2banservice that is running alongside the reverse proxy of a NextCloudinstance. Hence, I can give access to fail2ban to the logs from the remoteNextCloud and block connections in the reverse proxy not only based on its ownactivity, but also based on the activity in NextCloud itself.The following steps show how to modify the existing service for fail2ban to waitfor the folder with remote log files that is mounted with NFS: Edit the unit file $ sudo systemctl edit fail2ban.service Set RequiresMountsFor in the Unit section to the absolute pathneeded by the service [Unit] RequiresMountsFor=/var/log/nextcloud systemd will mount all mounts needed to access the path inRequiresMountsFor, even those with noauto." }, { "title": "Scrubbing a Btrfs filesystem", "url": "/posts/simple-btrfs-scrub/", "categories": "linux", "tags": "debian, storage, systemd", "date": "2021-09-18 12:00:00 +0200", "snippet": "Btrfs provides a tool called btrfs-scrubthat serves to scrub the filesystem. It can automatically read all data andmetadata blocks, verify checksums and repair corrupted blocks if there’s acorrect ...", "content": "Btrfs provides a tool called btrfs-scrubthat serves to scrub the filesystem. It can automatically read all data andmetadata blocks, verify checksums and repair corrupted blocks if there’s acorrect copy available.Periodic scrubbing Install btrfsmaintenance btrfs-scrub is provided by the btrfsmaintenance package, which is a project with extra maintenance tools developed by the btrfs maintainer and not found in btrfs-progs. This article focuses on the scrubber but there is also a defrag. $ apt install btrfsmaintenance Once installed, all provided services and timers should be disabled by default. Edit the configuration The configuration settings for all tools in btrfsmaintenance are centralized in /etc/default/btrfsmaintenance in Debian. Set the BTRFS_SCRUB_* variables in it as needed. Simplify the timer for btrfs-scrub By default, the timers are controlled through the btrfsmaintenance-refresh.service, which reads $BTRFS_SCRUB_PERIOD from the main configuration file. However, I prefer to simplify this setup by directly configuring the periodicity in the systemd timer for btrfs-scrub $ systemctl edit --full btrfs-scrub.timer Contents of btrfs-scrub.timer with fixed monthly execution (every 8th day at 5 AM) [Unit] Description=Scrub btrfs filesystem, verify block checksums Documentation=man:btrfs-scrub [Timer] OnCalendar=*-*-8 5:00:00 RandomizedDelaySec=1h Persistent=true [Install] WantedBy=timers.target Start the scrubbing timer $ systemctl enable btrfs-scrub.timer $ systemctl start btrfs-scrub.timer The time of next execution can be checked with the command systemctl list-timers --all Manual raw scrub of the driveAlternatively, if any underlying drive has faulty blocks, those can beidentifies with badblocks and manually evicted with hdparm: Tests from S.M.A.R.T. Check the test reports from the SMART system in the drive to verify its health status. For instance, the sda drive in our affected system reports a read failure $ smartctl -l selftest /dev/sda smartctl 6.6 2017-11-05 r4594 [armv7l-linux-5.10.34-mvebu] (local build) Copyright (C) 2002-17, Bruce Allen, Christian Franke, www.smartmontools.org === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error # 1 Short offline Completed: read failure 20% 30351 4409 [...] Identify bad blocks Check the block size of the drive with fdisk -l and execute badblocks to identify all faulty blocks in the disk. For instance, in our sda drive with blocks of 512 bytes $ badblocks -b 512 /dev/sda [...] 4409 Verify state of reported bad blocks $ hdparm --read-sector 4409 /dev/sda Repair bad blocks This step will not recover any data! The faulty blocks will be disabled from the drive and it will (probably) continue to function with the remaining healthy blocks. The filesystem will only be able to recover the lost data if there is redundancy. $ hdparm --yes-i-know-what-i-am-doing --repair-sector 4409 /dev/sda " }, { "title": "NFS mount in Ubuntu Core 18", "url": "/posts/nfs-mount-ubuntu-core-18/", "categories": "nextcloud", "tags": "ubuntu, snap, storage", "date": "2021-08-15 12:00:00 +0200", "snippet": "Ubuntu Core is based onsnaps, which means that all software (even the systemkernel!) has to be provided by a snap container. The main consequence of thisdesign is that the system lacks many tools c...", "content": "Ubuntu Core is based onsnaps, which means that all software (even the systemkernel!) has to be provided by a snap container. The main consequence of thisdesign is that the system lacks many tools commonly found in other Linux distros(eg wget), rendering a simple task such as mounting a NFS volume to be nottrivial.The good, the bad and the ugly Real solution: use Ubuntu Server instead of Ubuntu Core if you need NFSmounts, the base of the system is the traditional Ubuntu with its commonarray of tools and it is possible to use snaps as well. Time sink solution: make your own snap for nfs-common, the kernel in UbuntuCore already provides support for NFS. The only missing pieces are the NFStools in userland. Quick solution: use BusyBox, it can be easilyside-loaded into the system and provides support for NFS.Install BusyBox into Ubuntu Core 18The easiest approach is to download BusyBox into some local machine and transferthe tarball to the Ubuntu Core system with ssh. However, in the following I’llshow a solution that only relies on the Ubuntu Core system itself: Install the classic snap The classic snap allows to run a classic Ubuntu environment that includesapt. Beware that it was released for Ubuntu Core 16 and it has not beenupdated for v18. Nonetheless, we will just use it to download BusyBox into thesystem and unistall it afterwards $ snap install classic --edge --devmode classic (beta) 16.04 from Canonical✓ installed Channel latest/edge for classic is closed; temporarily forwarding to beta. Running in devmode lightensthe confinement of the snap, giving it access to the host system. Install GNU Wget within snap classic $ sudo classic Creating classic environment [...] (classic) $ sudo apt install wget Download BusyBox Since my system runs on a Raspberry Pi 4 rev B, I took the build forarmv8l. Choose the one most appropriate for your system frombusybox.net. (classic) $ wget https://www.busybox.net/downloads/binaries/1.31.0-defconfig-multiarch-musl/busybox-armv8l The downloaded busybox binary can be moved out of the classic snap as we arerunning in devmode. (optional) Uninstall classic The classic snap will not be needed in the following sections $ sudo snap remove classic Mount NFS volume with BusyBoxExecute mount as usual from within the busybox binary image$ chmod u+x /root/busybox-armv8l$ sudo /root/busybox-armv8l mount -t nfs4 -o rw nfs_server:/cloud /media/nextcloudAutomatize NFS mount with systemdWe can create a simple systemd service to emulate the corresponding systemd unitmount for the NFS volume. The custom service will mount/unmount the NFS volumeon system boot/shutdown using BusyBox[Unit]Description=Mount NFS volume with Nextcloud mediaWants=network-online.targetAfter=network-online.target[Service]Type=simpleRemainAfterExit=yesExecStartPre=/bin/mkdir -p /media/nextcloudExecStart=/root/busybox-armv8l mount -t nfs4 -o rw nfs_server:/cloud /media/nextcloudExecStop=/root/busybox-armv8l umount /media/nextcloud[Install]WantedBy=remote-fs.targetNote: ExecStartPre ensures that the mount point for the NFS volume exists onservice start.Use the NFS volume in NextCloudAt this point, you can see that I carried out this process to overcome thelimitations in a NextCloud instance installed from aSnap package.These are the steps to use the NFS volume as the data storage of the NextCloudsnap: Enable access to /media to NextCloud snap $ sudo snap connect nextcloud:removable-media Change the data directory in NextCloud by following the steps in theNextCloud Wiki Folders in /media are not persistent across reboots." }, { "title": "Updating variables in Linux shell environment", "url": "/posts/updating-variables-in-linux-shell-environment/", "categories": "linux", "tags": "bash", "date": "2021-07-24 12:00:00 +0200", "snippet": "Manipulating environment variables can be tedious as it is almost always necessary to check if the target variable already exists, its value and act in consequence. Fortunately, shell parameter exp...", "content": "Manipulating environment variables can be tedious as it is almost always necessary to check if the target variable already exists, its value and act in consequence. Fortunately, shell parameter expansion in bash provides an effective mechanism to quickly deal with those checks and it is specially useful in handling default values and expanding lists of paths.Default valuesThe minus - operator in the expansion will substitute the preceding parameter if it is unset with the result of the expansion of the word after it. Hence, - allows to easily assign default values to any shell variables. It is also possible to use an extra colon in the operator :- to substitute parameters that are unset or null.PARAMETER=${PARAMETER:-default}Expand a list of pathsEnvironment variables defining a collection of paths (i.e. $PATH, $LD_LIBRARY_PATH) are usually formed as a simple colon-separated list of paths. In this case, the goal is to set or update those variables avoiding any trailing : characters.The plus + operator in the expansion only performs the substitution if the preceding parameter already exists. Such a substitution can be used on the target list of paths to automatically prepend/append an extra colon. Therefore, combining this substitution with any path in the definition of the shell variable will cleanly set or update it with the new path.# Prepend path if $PATH existsexport PATH=${PATH+${PATH}:}$HOME/new/bin# Append path if $PATH existsexport PATH=$HOME/new/bin${PATH+:${PATH}}" } ]
